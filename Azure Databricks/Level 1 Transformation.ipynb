{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Broze To Silver Transformation (Level 1)\n",
    "\n",
    "This Notebook's purpose is to **transform** the raw data in ***bronze*** conatainer and **store** it onto the ***silver*** container for the further processing.  \n",
    "In this **Level 1** notebook basic transformation are performed, which includes \n",
    "1. Identification & Removal of **Null** values.\n",
    "2. **Feature Selection** for reduced dataset.\n",
    "3. Modification of **colomns** names.\n",
    "4. **Type Casting** of **'date'** colomn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "80478204-8b50-4571-aa48-977fe7623d82",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Displays the files & folder of SalesLT folder in bronze container.\n",
    "dbutils.fs.ls(\"/mnt/bronze/SalesLT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c70087f2-49e6-4803-bd20-0a4d93648d1b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Listing the files in silver container which will be empty for now.\n",
    "dbutils.fs.ls(\"/mnt/silver\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since each file is stored under it's own specific folder, creating a variable *'table_names'* which containes all the table names.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1c3e1142-c02f-4d26-9a7d-0071ea445e0a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "table_names = []\n",
    "\n",
    "for i in dbutils.fs.ls(\"/mnt/bronze/SalesLT\"):\n",
    "    table_names.append(i.name.split('/')[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we can't transform all tables at once, Each table is *loaded* as dataframe *transformed* and *stored* onto the ***silver*** container. This goes for all tables one after the other.\n",
    "\n",
    "To automate the transformations for all tables in one run, *For Loop* is used to iterate over *table_names*.\n",
    "\n",
    "In the dataset to change the date column format from ***timestamp*** to ***YYYY-MM-DD***, *For Loop and IF condition* is used to find the columns which contains string *'date' Or 'Date'* in it. which then is changed into requied format i.e, ***YYYY-MM-DD***.\n",
    "\n",
    "Once the ***df*** is transformed it is stored onto ***silver*** container accordingly. \n",
    "\n",
    "To Store/Load the tranformed df, ***write*** method is used where format option is set to ***delta***.  \n",
    "Here ***mode*** can have two values:\n",
    "- ***overwrite*** - Replaces all the existing data with the current data.\n",
    "- ***append*** - Appends/Adds the current data to already existing data. It is mainly used for increamental load\n",
    "\n",
    "Since this project does not follow increamental load, the option is set to ***overwrite***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "506cca73-344e-43c4-9b46-cb2b4ecdf075",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import from_utc_timestamp, date_format, col\n",
    "from pyspark.sql.types import TimestampType\n",
    "\n",
    "for table in table_names: # To iterate over the tables\n",
    "    table_path = \"/mnt/bronze/SalesLT/\" + table + \"/\" + table + \".parquet\"\n",
    "    df = spark.read.format('parquet').load(table_path)\n",
    "    columns = df.columns\n",
    "\n",
    "    for column in columns: # To iterate over the columns of each tabel\n",
    "        if 'Date' in column or 'date' in column: # Finds columns which has 'Date' or 'date' string.\n",
    "            df = df.withColumn(column, date_format(from_utc_timestamp(col(column).cast(TimestampType()), 'UTC'), 'yyyy-MM-dd')) # Converting timestamp to date format\n",
    "\n",
    "    output_path = \"/mnt/silver/SalesLT/\" + table + \"/\" # Loding path in silver container.\n",
    "    df.write.format('delta').mode('overwrite').save(output_path) # Loading/Overwriting the data"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Level 1 Transformation",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
